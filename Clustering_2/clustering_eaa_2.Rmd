---
title: "Advanced Concepts of Clustering in Insurance: Clustering of Marine Losses"
output:   
  html_notebook:
    toc: true
    number_sections: true
    theme: readable
    toc_float:
      collapsed: false
      smooth_scroll: false
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

# Primer

This is an example on how to use clustering techniques to analyze insurance data, not a blue print or best practice that can be followed for any other analysis. Any data analysis, in particular in unsupervised learning, relies heavily on domain knowledge and the application at hand. Even for this data set several other reasonable approaches could have been taken given a different set of assumptions and goals.

# The dataset

This is an [CAS datasets](http://cas.uqam.ca/pub/web/CASdatasets-manual.pdf). From the official description:

*"The univariate dataset was collected by an unknown French private insurer and comprise 1,274 marine losses between the January 2003 and June 2006. The status of the claim (settled or opened) is determined at the end of June 2006."*

> THUMBS UP: Who of you has experience in marine insurance business?


```{r message=FALSE, warning=FALSE}
# install.packages("CASdatasets", repos = "http://cas.uqam.ca/pub/", type="source")
library(CASdatasets)
seed_var <- 03052021
```



```{r}
data(fremarine)
```


```{r}
summary(fremarine)
```


We put us into the position of a fictional claims analyst that wants to understand the losses better, having the following questions / whishes in mind:

* Being new in the position, I do not have a good overview of recent losses. Previous analyses where focused on single cases rather then the overall portfolio.
* With a segmentation of our recent losses I could the monitor the loss development over time.

* Unfortunately, we are not enough people to review each claim in depth. New claims that do not fit very well to the existing segmentation, e.g., are quite far away from all centroids, could be marked as outliers and trigger a detailed assessment.

* Claims handlers would like to gain efficiency by specialising on certain portfolio segments (large ships vs small etc.)

* My underwriting colleagues would like to understand our losses better but many of them do not have a quantitative background. With a few representative examples (e.g., cluster centroids), I could give them a simplified but complete overview over our recent losses.


## Data set dictionary

The data set is of mixed data type:

```{r}
sapply(fremarine,FUN=class)
```

A (somewhat brief) explanation can be found in the manual and is shown below:

Date variables: 

* OccurDate: The day of claim occurence
* ReporDate: The day of claim reporting.ShipCategThe category of the insured ship

Factor variables:

* ShipBrand: The brand of the insured ship
* ShipHull: The hull of the insured ship
* Departement: The French region in which the ship is headquartered

Numeric variables:

* ShipPower:The power of the insured ship
* ShipEngNb: The engine number of the insured ship
* ShipEngYear: The engine year of the insured ship
* ShipBuildYear: The building year of the insured ship
* ShipLength: The length of the insured ship
* ShipTonnage: The tonnage of the insured ship
* InsuredValue: The insured value of the insured ship
* ClaimPaid: The paid amount (EUR) of the claim
* ClaimStatus: The status of the claim. Unsettled


### Assumptions

For simplicity, we assume the paid claims includes the final amount paid to the policyholder after deductible, recourse etc. We will also restrict our analysis to settled claims only for which we have complete and final information.

Additionally, we decide not to use the date since:

* Our (fictional) claims colleagues assure that our recent data set is quite homogeneous in time, there should not be strong trends.
* New claims, obviously, have a date outside of the existing range making it hard to segment them otherwise.

Thus we arrive at the following basis for our analysis:

```{r, message=FALSE, warning=FALSE}
library(data.table)
cat_vars <- c("ShipBrand","ShipHull","Departement")
num_vars <- c("ShipPower","ShipEngNb","ShipEngYear","ShipBuildYear","ShipLength","ShipTonnage","InsuredValue","ClaimPaid")
dat4clustering <- as.data.table(fremarine)
dat4clustering <- dat4clustering[ClaimStatus=="settled",mget(c(num_vars,cat_vars))]
```


### Data preview

```{r, message=FALSE, warning=FALSE}
library(DT)
datatable(dat4clustering, filter = 'top', options = list(pageLength = 5, autoWidth = TRUE))
```


## Fictional Setting

For educational reasons, we imagine 3 different settings:

1. The data set does only contain numeric variables and no missings.
2. The data set does only contain numeric variables but some missings.
2. The data set contains numeric and categorical variables.

# Data preparation


## Numeric features{.tabset}


### Pre-processing

We remove categorical variables, missings and ensure that we do not introduce any duplicates

```{r}
dat_num_no_missings = unique(na.omit(dat4clustering[,mget(num_vars)]))
```

Here we use the Euclidean metric here. Since the numeric variables are measured on different scales (years, counts, EURs, sizes, etc.) we have to center and scale the data:

```{r}
dat_num_no_missings_scaled = scale(dat_num_no_missings)
```

### Descriptive analysis

First look at the data:

```{r, message=FALSE, warning=FALSE}
library(ggridges)
library(ggplot2)

data4ridgeplot <- melt(as.data.table(dat_num_no_missings_scaled))
ggplot(data4ridgeplot) +  geom_density_ridges(aes(x = value, y = variable)) + theme_ridges() + theme(legend.position = "none") + xlim(-2.5,2.5) # given a normal distribution ~1% should be outside of this range
```

Typically, variables with a large variance turn out to be more important differentiators. Also correlations should be checked.

> YES OR NO: Do you expect large correlations?

```{r, fig.width=12, fig.height=12, warning=FALSE, message=FALSE}
cor_mat <- cor(dat_num_no_missings_scaled, method="spearman")
library(corrplot)
corrplot.mixed(cor_mat,order="hclust",lower.col = "black")
```

Ship length and tonnage are (unsurprisingly) almost perfectly correlated, such as length and insured value or power. Domain knowledge can often be used to reduce trivial correlations (e.g., in life insurance: BMI instead of height and weight)


A combination of histograms and correlations can be obtained using a pairs plot (using Pearson correlation):

```{r, warning=FALSE}
library(GGally)
dat_num_no_missings_scaled_df <- as.data.frame(dat_num_no_missings_scaled)
ggpairs(dat_num_no_missings_scaled_df[,c("ShipLength","ShipTonnage")], progress=FALSE)
```


## Mixed-type Data {.tabset}

We'll filter out NAs here and focus only on the clustering part. In practice, you could use mice  to complete the data, but it may be a good starting point to first consider only available data.

```{r}
dat_no_missings = unique(na.omit(dat4clustering[,mget(c(num_vars,cat_vars))]))
```

As before the numerical variables have to be scaled with scale() as above. Then we re-add the categorical features again.

```{r}
dat_no_missings_scaled <- as.data.table(scale(dat_no_missings[,mget(num_vars)]))
dat_no_missings_scaled[,ShipBrand:=dat_no_missings$ShipBrand]
dat_no_missings_scaled[,ShipHull:=dat_no_missings$ShipHull]
dat_no_missings_scaled[,Departement:=dat_no_missings$Departement]
```


# Clustering algorithms {.tabset}

## K-Means

> YES OR NO: Ever worked with k-means before?

### Numeric data

We will perform a K-means Cluster Analysis using the flexclust package here. Below we run clustering algorithms repeatedly for different numbers of clusters and returns the minimum within cluster distance solution for each. The sum of within cluster distances which is decreasing with k (since between cluster distances are not counted). Ideally we'd look for an elbow / hockey stick.

```{r message=FALSE}
library(flexclust)
k_try <-2:15
find_k <- stepFlexclust(dat_num_no_missings_scaled,k_try,nrep=5,verbose=FALSE, seed=seed_var, FUN=cclust)
plot(find_k)
```

Based on the result and our applications we select 7 clusters.

```{r}
nr_clusters = 7
cclust_res <-  getModel(find_k,nr_clusters-1)
cclust_res
```

### Mixed data types

One can use the clustMixType as in the first part of the course which uses the k-prototypes algorithm.  Alternatively, one can use partitioning around medoids, where one computes the distance matrix using Gower's distance a-priori. However, this approach is rather slow (and memory intensive) and works only for smaller data sets. We will show an example of a clustering based on a pre-computed distance matrix later in these notes.


## Hierarchical dbscan

> YES OR NO: Do you expect the result of h-dbscan to be similar to the one for k-means?

Here we are using a variant of DBSCAN, hierarchical DBSCAN. It has the benefit that we only have to specify the minPts parameter. Note that not all points are clustered, cluster zero contains all outliers but will not be a homogeneous set of similar points. 

We want a cluster to contain at least 7 points, thus we set minPts=7. By playing around with this parameter, we see that a larger number produces very few clusters which would not be helpful, and a lower number leads to a lot of outliers. We observe that most points fall into cluster 2. Such a result would not be quite practical if we need segments for our claims colleagues to specialize on. However, it does provide us with a few "uncommon" examples and 48 outlier, we'll investigate at a later stage.

```{r}
library(dbscan)
hdbscan_res <- hdbscan(dat_num_no_missings_scaled, minPts = 7)
hdbscan_res
```

HDBSCAN essentially computes the hierarchy of all DBSCAN* clusterings, meaning for all (reasonable) values of eps , the radius of the epsilon neighborhood. Then it uses a stability-based extraction method to find optimal cuts in the hierarchy, thus producing a flat solution. Depending on a different value for eps, we could get more or less clusters using the dbscan() function from the same package.

```{r}
plot(hdbscan_res, show_flat=T)
```

A higher value of eps would result in only 4 clusters where cluster 4 and 5 will be merged. A lower value will quickly lead to a fine segmentation of cluster 2 with lots of new clusters and potentially, also outliers.

The result seems to be of less use for this data - most ships fall into cluster 2, the other categories are sparsely populated.

Similarly as with k-means, data with mixed data types can per se be used with DBSCAN if the pairwise distances are computed a-priori, which is normally not feasible for large data sets.


## Model-based clustering


### Gaussian Mixture Model for numerical data

> CHAT: When did Carl Friedrich Gauß define the Normal/Gaussian distribution? (answer at the end of this section)

Here we are making the assumption that each point, given the true cluster assignment, is follows a Gaussian distribution. This stochastic framework allows us to determine the clusters via Maximum Likelihood and evaluate the fit using statistical measures, such as the BIC (Bayesian Information Criterion), which is defined as nr_parameters * log(nr_observations) minus 2 times the log-likelihood.

We fit configurations with 1 to 10 clusters, hence max_clusters=10. Ensure that the number of points >> number of components. 
Most other parameters are used to control the algorithm, that we do not investigate in detail. In practice, some sensitivity analysis will quickly show if deviating from the defaults makes a difference or not. Ideally, one at least observes convergence for large values of km_iter and em_iter. If not, the algorithm may be unstable and not fit for the data at hand.


```{r, message=FALSE}
library(ClusterR)
```


```{r}
opt_gmm = Optimal_Clusters_GMM(dat_num_no_missings_scaled, max_clusters = 10, 
                               criterion = "BIC", dist_mode = "eucl_dist", 
                               seed_mode = "random_subset", seed= seed_var,
                               km_iter = 10, em_iter = 10, var_floor = 1e-10,
                               plot_data = T)
```

The fit becomes better if we increase the number of clusters from one, but eventually the improvement is lower than the penalization in the BIC formula. We'll decide to use 7 clusters since this value is quite close to the minimum and we want to have a rather low number of clusters for our applications.


```{r}
gmm_res = GMM(dat_num_no_missings_scaled,gaussian_comps  =  7, dist_mode = "eucl_dist", 
              seed_mode = "random_subset", km_iter = 10, em_iter = 10, seed = seed_var)   
```

We define a wrapper to easily obtain predictions from our Gaussian Mixture Model (GMM)

```{r}
predictGMM <- function(x,dat) {
  centroids = x$centroids
  cov = x$covariance_matrices
  w = x$weights
  return (predict_GMM(dat,centroids,cov,w))
}
```

The resulting clusters are more evenly distributed 

```{r}
pred_gmm_res <- predictGMM(gmm_res,dat_num_no_missings_scaled)
summary(factor(pred_gmm_res$cluster_labels))
```

Since we deal with probabilistic models, we can compute the probability that each point x belongs to cluster k. Thus one often speaks of "soft clustering". As long as no decision is taken, each point belongs to each cluster with a certain probability. Naturally, one assigns each point to the most likely cluster. We observe that the probabilities of the most likely cluster are nicely skewed to the right, indicating a clear cluster assignment for most points.

```{r}
probab_of_majority_class <- apply(pred_gmm_res$cluster_proba,1,max)
hist(probab_of_majority_class)
```

> YES OR NO: The 7 cluster distributions have independent components?


```{r}
gmm_res$covariance_matrices
```

What appears like a non-diagonal covariance matrix, is in fact something else: Each cluster distribution lives in an `dim(dat_num_no_missings_scaled)[2]` dimensional space, since one needs one dimension per variable, and has a diagonal covariance matrix stated in the rows. So the answer is yes, they do have independent components.

Note that the 2nd component of many clusters has a zero variance, thus there is no variation. The 2nd component refers to the 2nd feature, the number of engines.

*Gauß defined the Normal distribution in 1809 in his work "Theoria motus corporum coelestium in sectionibus conicis solem ambientium", which you can surprisingly even buy on Amazon.*


### Mixed data types

For data sets with numerical and categorical features, we assume a multivariate distribution that is  Gaussian for numerical components and follows a Dirichlet Distribution (that is a multivariate extension of the beta distribution) for categorical features.


```{r, message=FALSE, warning=FALSE}
library(MixAll)
```

In contrast to the previous setting with only numerical variables, the optimal number of clusters is 5.

```{r}
ldata = list(dat_no_missings_scaled[,mget(cat_vars)],
             dat_no_missings_scaled[,mget(num_vars)])
lnames = c("categorical_pk_pjk","gaussian_pk_sjk")
set.seed(seed_var)
clustermix_res <- clusterMixedData(ldata, lnames, 
                                   nbCluster = 5:9, 
                                   strategy = clusterFastStrategy(),
                                   criterion = "BIC")
summary(clustermix_res)
```
There is no need to re-fit the model with this package:

```{r}
pred_clustermix <- clustermix_res@zi
# pred_clustermix <- clusterPredict(ldata,clustermix_res) # for new data one would need the predict function
table(pred_clustermix)
```

> 3 minute BREAK. Have a bio break if you need one, get a tea / coffee or simply a glass of water.

# Cluster evaluation and interpretation {.tabset}

## Cluster evaluation metrics

We'd focus on k-means and GMM here since hdbscan produced essentially only one large cluster. Moreover, cluster 0 cannot be treated as a cluster since it is merely a collection of outliers. To make a somewhat fair comparison one would have to take out all rows belonging to the outlier cluster 0 before calculating the metrics (thereby accepting that these points are really outliers).


### Adjusted RandIndex

The RandIndex goes through all pairs of observations and counts which fraction of them are both in the same cluster or both in different ones, in the k-means or the GMM segmentation (even when the clusters are different, for such a pair the segmentation is consistent since both observations are considered similar or un-similar in each segmentation).

The adjusted Rand index accounts for the fact that some overlap might be just to random chance, and establishes a baseline by using the expected similarity of all pairwise comparisons between clusterings obtained by a random clustering

There is a rather low consistency between the result from GMM and k-Means, as the adjusted RandIndex shows. The different assumptions lead to quite different segmentations.

```{r}
external_validation(pred_gmm_res$cluster_labels,flexclust::predict(cclust_res),
                    method = "adjusted_rand_index")
```


### Davies-Bouldin


```{r, message=FALSE, warning=FALSE}
library(clusterSim)
```

The definition of the Davies-Bouldin (DB) index of a segmentation is a bit involved. The dispersion (or variation) within cluster $i$ can be defined as $ S_i = (\sum_{x \in C_i} |x-c_i|^q)^{1/q} $, where $c_i$ denotes the centroid of the cluster. For $q=2$ this corresponds to the standard deviation within cluster $i$ since the centroid is defined as the mean.

A measure of distance between the centroids of two different clusters can be computed by 
$ R_{i,j} = (\sum_{k=1}^n |c_{i,k}-c_{j,k}|^p)^{1/p} $. For $p=2$ this is simply the Euclidean distance between the centroids.

The separation of cluster $i$ from all other cluster can then be computed by $r_i = max_{j\neq i}\frac{S_i+S_j}{R_{i,j}}$, that is, for the "worst case" pairing with cluster $i$, the ratio between the sum of within cluster dispersions and the between cluster distance.

The DB index is then defined as the average over all clusters
$DB = \frac{1}{k}\sum_{i=1}^k r_i$. Thus one computes, for the "worst case" pair of clusters, the 

Consequently, a lower DB implies a better separation: In such a situation, all clusters are rather far apart and tightly centered around their mean.

```{r}
cclust_db <- index.DB(dat_num_no_missings_scaled, flexclust::predict(cclust_res),
                      p=2, q=2)
cclust_db$DB
```


There is a higher value for GMM, meaning that k-means provides a better segmentation.

```{r}
gmm_db <- index.DB(dat_num_no_missings_scaled, pred_gmm_res$cluster_labels, p=2, q=2)
gmm_db$DB
```

## Vizualization using T-SNE

Visualization of high-dimensional data into 2 (or 3) dimension can be done via linear (Principal Component Analysis, Multidimensional Scaling) or non-linear projection methods. While non-linear methods often achieve better projections, they are computationally more costly and have several hyper parameters.

T-SNE maps a set of points from a high-dimensional space in a lower-dimensional
Mapping should preserve the local neighbourhood structure of each point by minimizing Kullback-Leibler divergence between the two distributions


```{r, message=FALSE, warning=FALSE}
library(Rtsne)
```

```{r}
set.seed(seed_var)
tsne_out <- Rtsne(dat_num_no_missings_scaled,pca=FALSE, perplexity = 30)
```

Within this (non-linear) 2-dimensional projection, we see that indeed not all clusters are terribly well separated.

```{r}
data_tsne <- as.data.table(tsne_out$Y)
ggplot(data_tsne,aes(x=V1,y=V2,color=factor(predict(cclust_res)))) + geom_point() + guides(color=guide_legend(title="Cluster")) +
scale_color_brewer(palette = "Set2")
```

> CHAT: Any ideas how to enhance this visual inspection with (quantitative) evaluation metrics?

We can also connect this with intermediate calculation results of the Davies-Bouldin index. Let us consider Cluster 4, for example.The centroid of this cluster is much closer to the centroid of Cluster 7 than to all other cluster centroids. 

```{r}
cclust_db$d[,4] # matrix of distances between centroids or medoids of clusters
```

Some points are far away from the bulk (and close to Cluster 3), hence the dispersion of this cluster is rather high compared to the other clusters (actually highest).

```{r}
cclust_db$S
```

Looking at the R matrix, Cluster 4 is indeed not so well separated from Cluster 3 and 7.

```{r}
cclust_db$R
```
All in all, in the worst case is aggregatio (max of R matrix) is has a lower index than Cluster 1 and 6, for example, which visually show a rather high dispersion and are close to other clusters.

```{r}
cclust_db$r
```

With descriptive analyses one can (e.g. histograms and correlations) one can better understand the clusters and their similarity. A bad separation of some clusters can also motivate a re-clustering with a lower number of clusters (investigate clusters with an $r$-value that is much higher than the average, i.e., the DB index.)

Of course, we can do the same plot for GMM. We only have to define a helper function the maps the cluster object and a data table to a vector with integers, the cluster assignments per row of the data table.

```{r}
predictGMM_labels <- function(gmm,newdata){
  return (predictGMM(gmm,newdata)$cluster_labels)
}
```

```{r}
data_tsne <- as.data.table(tsne_out$Y)
ggplot(data_tsne,aes(x=V1,y=V2,color=factor(predictGMM_labels(gmm_res,dat_num_no_missings_scaled)))) + geom_point() + guides(color=guide_legend(title="Cluster")) +
  scale_color_brewer(palette = "Set3")
```

## Feature Importance

> CHAT: What would you expect to be the most relevant variable? Meaning that if we randomly permute this variable, the clustering changes the most.

Permutation based approach to determine the overall relevance of a feature for the segmentation. Parallels the approach taken in classification tasks.

```{r message=FALSE, warning=FALSE}
library(FeatureImpCluster)
```

We observe that build and engine year have the highest importance, while claims paid is irrelevant for the clustering result.

```{r}
set.seed(seed_var)
FeatureImp_cclust <- FeatureImpCluster(cclust_res,as.data.table(dat_num_no_missings_scaled))
plot(FeatureImp_cclust)
```

It is very interesting to see the the clusters from GMM rely on quite different variables. Here Length and Tonnage are most relevant, also InsuredValue and ClaimPaid define the segments. On the other hand, ship and engine year have a very low relevance.

```{r, eval=FALSE}
set.seed(seed_var)
FeatureImp_gmm <- FeatureImpCluster(clusterObj = gmm_res,
                                    data = as.data.table(dat_num_no_missings_scaled),
                                    predFUN = predictGMM_labels)
plot(FeatureImp_gmm)
```


## Interpretation using rules-based ML models

A simple supervised machine learning algorithm may be easier to interpret than a complex unsupervised algorithm. We train this model on the cluster labels. 

> CHAT: Which supervised ML models do you know that you would call interpretable?

C5.0 can create an initial tree model then decompose the tree structure into a set of mutually exclusive rules. These rules can then be pruned and modified into a smaller set of potentially overlapping rules. The rules can be created using the rules option.

```{r, message=FALSE, warning=FALSE}
library(C50)
```

In the context of machine learning, a rule is a conditional logical statement that can be attached to a predicted value. Note that the conditions have to be considered in the printed order.

```{r}
set.seed(seed_var)
gmm_rule <- C5.0(x = dat_num_no_missings_scaled, 
                 y = factor(pred_gmm_res$cluster_labels), rules = TRUE)
summary(gmm_rule)
```

Interestingly, the attribute usage provides another source to measure variable importance. The lift values can be interpreted as an importance of the rule.

The evaluation of our algorithm should be taken with case, since we are using the training set. The performance can be much worse on a table of new ships. On our trainng set, only 14 ships are not fitted to the cluster they were originally assigned to.

Such an analysis is not limited to C.50, one could use decision tree or a GLM as well.


# Dimensionality reduction

For production or interpretation, reducing the the problem to a lower dimensional space can be helpful. What does this mean in a cluster analysis? We will show two (out of many) options:

1. One can select a subset of features that are most important for the clustering assignment
2. One can cluster the features to obtain groups of similar features

In any case, this allows to re-compute the clusters on a lower dimensional space.

## Feature Mapping

We start by first computing the distance between the features, therefore we need to transpose the data matrix.

```{r}
dist_dat <- dist(t(dat_num_no_missings_scaled), method = "euclidean")
dist_dat
```

Based on this distance matrix, we apply a hierarchical clustering.

```{r}
hc_res <- hclust(dist_dat)
plot(hc_res)
```

Sine we normalized our data, all features are on the same scale, so we could aggregate features by taking the mean, for example. Thus, in order to reduce the dimension from 8 to 5, we could average Length and Tonnage to a single variable that we call ShipSize. 

```{r, message=FALSE, warning=FALSE}
library(dplyr)
```


```{r}
dat_num_aggregated <- data.frame(dat_num_no_missings_scaled)
dat_num_aggregated$ShipSize = 
  (dat_num_aggregated$ShipLength + dat_num_aggregated$ShipTonnage)/2
dat_num_aggregated <- dat_num_aggregated %>% select(!c(ShipLength,ShipTonnage))
```

Note that the values of this variable can not be interpreted anymore in a meaningful way. In principle, this would also allow us to cluster a ship if one of the values is missing, in this case we would simply put full weight on the other variable. 

> YES OR NO: Do you expect the new segmentation based on Size instead of Length & Tonnage to be quite similar to the old one such that the RandIndex comparing both segmentations is near to one, say > .9 ?

```{r}
set.seed(seed_var)
cclust_org_res <- cclust(dat_num_no_missings_scaled,k=7) # eliminate impact of the random seed
set.seed(seed_var)
cclust_aggr_res <- cclust(dat_num_aggregated,k=7)
```

Despite a negligible reduction (improvement) in the DB index. 

```{r}
cclust_aggr_db <- index.DB(dat_num_aggregated, flexclust::predict(cclust_aggr_res),
                      p=2, q=2)
cclust_aggr_db$DB
cclust_org_db <- index.DB(dat_num_no_missings_scaled, flexclust::predict(cclust_org_res),
                      p=2, q=2)
cclust_org_db$DB
```

However, the segmentation differs quite a bit from the previous one based on the raw data.

```{r}
external_validation(flexclust::predict(cclust_org_res),
                    flexclust::predict(cclust_aggr_res),
                    method = "adjusted_rand_index")
```


## Feature Selection using Feature Importance

We used Feature Importance as a tool to better understand our clustering results. Here we benefit from it in another way: To de-select features which to not materially contribute to the clustering result.

Iterating the Feature Importance algorithm over various seeds and bootstrap distributions gives a more robust estimate of feature importance.

```{r}
# randomly sample starting seeds
set.seed(seed_var)
nr_seeds <- 5
sub <- 0.7 # 70% subset in each iteration
biter <- 5 # 5 bootstrap iterations
seeds_vec <- sample(1:10000,nr_seeds)

savedImp <- data.frame(matrix(0,nr_seeds,dim(dat_num_no_missings_scaled)[2]))
count <- 1
for (s in seeds_vec) {
  set.seed(s)
  res <- cclust(dat_num_no_missings_scaled,k=nr_clusters)
  set.seed(s)
  FeatureImp_res <- FeatureImpCluster(res,as.data.table(dat_num_no_missings_scaled),sub = sub,biter = biter)
  savedImp[count,] <- FeatureImp_res$featureImp[sort(names(FeatureImp_res$featureImp))]
  count <- count + 1
}
names(savedImp) <- sort(names(FeatureImp_res$featureImp))
```

Turns out that feature importance is quite stable. Claims paid might be removed from the data for clustering purposes.

```{r, fig.height = 5, fig.width = 11}
boxplot(savedImp)
```

```{r}
set.seed(seed_var)
cclust_noClaimPaid_res <- cclust(data.frame(dat_num_no_missings_scaled) %>% 
                                   select(-ClaimPaid),k=7)
```

Again there is some difference in the resulting segmentation. It would make sense to further investiage which rows (ships) are assigned to different clusters.

```{r}
external_validation(flexclust::predict(cclust_org_res),
                    flexclust::predict(cclust_noClaimPaid_res),
                    method = "adjusted_rand_index")
```


## Feature Selection in Model-based clustering

This seems to be super slow...?

ToDo: Run once on PC and persist results, or remove from script

```{r, eval=FALSE}
library(clustvarsel)
library(doParallel)
clustvarsel_res <- clustvarsel(dat_num_no_missings_scaled,G=nr_clusters,
                               search="greedy", direction="forward",
                               emModels1 = "E", emModels2 = "EII", # ?mclustModelNames
                               samp = TRUE, parallel = 3)
```


# Outlier detection

> CHAT: In a single sentence, how would you define the term "outlier"?

## Distance based (k-means)

We'll compute the distance of each data point to its cluster centroid to identify the top 5 points that are farthest away from it, and in that sense "special".

```{r}
centers_matrix <- cclust_res@centers[predict(cclust_res,dat_num_no_missings_scaled), ]
distances <- sqrt(rowSums((dat_num_no_missings_scaled - centers_matrix)^2))
outliers <- order(distances, decreasing=T)[1:5]
```


```{r}
print(outliers)
```

For example, data point 253 lies in cluster 4 but has a rather low insured value and a very high paid claim. That is something our claims analyst might want to analyze further.

```{r}
dat_num_no_missings_scaled[outliers,]
predict(cclust_res,dat_num_no_missings_scaled[outliers,])
```
In our T-SNE map we can highligh in which areas our outlier are lying.

```{r}
data_tsne$distances <- distances
ggplot(data_tsne,aes(x=V1,y=V2,color=distances)) + geom_point() + scale_color_gradient2()
```

Note that various other analysis can be made. For example, the share of outliers by cluster.

Also, the threshold "top 5" was completely arbitrary. If there are known outliers, this threshold can be calibrated in a way similar to how this is done for classification models (e.g. considering false positives / negatives on a validation set).

## Using DBSCAN

> CHAT: How many calculation are required to obtain the outliers in the DBSCAN segmentation ? (positive number)

DBSCAN is the only algorithm among the ones we are focusing here that has a build-in concept of outlier points. Hence no calculations are required.

```{r}
outlier_dbscan <- which(hdbscan_res$cluster==0)
outlier_dbscan
```

All 5 outliers from k-means are also outliers according to the DBSCAN methodology.

```{r}
outliers %in% outlier_dbscan
```


## Probabily-based (Model-based clustering)

Defintion of outlier: probability of belonging to assigned cluster is below 50% (this can happen since we have more than 2 clusters)

```{r}
outlier_gmm <- which(probab_of_majority_class<.5)
outlier_gmm
```

The outliers in GMM do not include the 5 outliers from k-means

```{r} 
outliers %in% outlier_gmm
```

And only the first one is also an outlier in DBSCAN.

```{r}
outlier_gmm %in% outlier_dbscan
```

All in all, we see that there is no universal concept of an outlier. It strongly depends on the clsutering methodology and how outlier is defined based on that.

Ideally, and unsupervised outlier detection approach is used to identify observations for manual inspection. The result of this inspection is tracked so that there will be, in the future, a data base to train a supervised learning algorithm.
